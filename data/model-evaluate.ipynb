{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d826cc79-1146-42ac-8dcd-d45fe90e0afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating base model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9aa41cef97423e8950a5b20e449115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [31:32<00:00,  9.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15163"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, torch, pickle, gc\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(\"test_reasoning_data_cache.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "count = 200\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def evaluate_model(model, processor):\n",
    "    model.eval()\n",
    "    predictions, references = [], []\n",
    "    for sample in tqdm(test_data[:count]):\n",
    "        image = sample[\"image\"]\n",
    "        gt = sample[\"messages\"][1][\"content\"][0][\"text\"]\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Analyze this medical image and provide step-by-step findings.\"}]}]\n",
    "        prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = processor(text=prompt, images=[image], return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, max_new_tokens=300)\n",
    "        decoded = processor.tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        predictions.append(decoded.strip())\n",
    "        references.append(gt.strip())\n",
    "    return rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"Evaluating base model\")\n",
    "processor = AutoProcessor.from_pretrained(\"google/medgemma-4b-it\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"google/medgemma-4b-it\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "base_scores = evaluate_model(model, processor)\n",
    "\n",
    "del model, processor\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c943bc3-78fa-486e-933f-5caac38aaaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Evaluating fine-tuned reasoning model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7254141a114e568009053eda10b0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [17:05<00:00,  5.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16085"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"🔍 Evaluating fine-tuned reasoning model\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Manusinhh/medgemma-finetuned-cxr-reasoning\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"Manusinhh/medgemma-finetuned-cxr-reasoning\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "ft_scores = evaluate_model(model, processor)\n",
    "\n",
    "del model, processor\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54c5bf2-ea63-4479-aed5-88c2b9dcb953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 ROUGE Score Comparison (on 2 samples):\n",
      "Metric       | Base Model | Fine-tuned\n",
      "--------------------------------------\n",
      "rouge1       | 0.2716     | 0.4299    \n",
      "rouge2       | 0.0521     | 0.1862    \n",
      "rougeL       | 0.1472     | 0.2806    \n",
      "rougeLsum    | 0.2588     | 0.4147    \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n📊 ROUGE Score Comparison (on 2 samples):\")\n",
    "print(f\"{'Metric':<12} | {'Base Model':<10} | {'Fine-tuned':<10}\")\n",
    "print(\"-\" * 38)\n",
    "for metric in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]:\n",
    "    b = base_scores[metric]\n",
    "    f = ft_scores[metric]\n",
    "    print(f\"{metric:<12} | {b:<10.4f} | {f:<10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99efd3e8-bf17-4ebf-9ee4-542c1675eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "base_vals = [base_scores[m] for m in metrics]\n",
    "ft_vals = [ft_scores[m] for m in metrics]\n",
    "\n",
    "# Create bar chart\n",
    "x = range(len(metrics))\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x, base_vals, width=0.4, label=\"Base Model\", align='center')\n",
    "plt.bar([i + 0.4 for i in x], ft_vals, width=0.4, label=\"Fine-tuned\", align='center')\n",
    "plt.xticks([i + 0.2 for i in x], metrics)\n",
    "plt.ylabel(\"ROUGE F1 Score\")\n",
    "plt.title(\"ROUGE Score Comparison (Base vs Fine-tuned)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rouge_comparison.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
